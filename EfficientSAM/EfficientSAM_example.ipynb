{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhhr4iSQuQq_"
      },
      "source": [
        "#Efficient SAM Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIrAUKnLClPD"
      },
      "source": [
        "This script provides example for how to get visualization result from EfficientSAM using ready-to-use torchscript, part of the code is borrow from MobileSAM project, many thanks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zylNfpYIuXeR"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "I64YhiKsS2KU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torchvision.transforms import ToTensor\n",
        "from PIL import Image\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw_4lyT8uMvy"
      },
      "source": [
        "#Box and Point prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4hrhmpHroFUH"
      },
      "outputs": [],
      "source": [
        "def run_ours_point(img_path, pts_sampled, model):\n",
        "    image = cv2.imread(img_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    img_tensor = ToTensor()(image)\n",
        "    pts_sampled = torch.reshape(torch.tensor(pts_sampled), [1, 1, -1, 2])\n",
        "    max_num_pts = pts_sampled.shape[2]\n",
        "    pts_labels = torch.ones(1, 1, max_num_pts)\n",
        "\n",
        "    predicted_logits, predicted_iou = model(\n",
        "        img_tensor[None, ...].cuda(),\n",
        "        pts_sampled.cuda(),\n",
        "        pts_labels.cuda(),\n",
        "    )\n",
        "    predicted_logits = predicted_logits.cpu()\n",
        "    all_masks = torch.ge(torch.sigmoid(predicted_logits[0, 0, :, :, :]), 0.5).numpy()\n",
        "    predicted_iou = predicted_iou[0, 0, ...].cpu().detach().numpy()\n",
        "\n",
        "    max_predicted_iou = -1\n",
        "    selected_mask_using_predicted_iou = None\n",
        "    for m in range(all_masks.shape[0]):\n",
        "        curr_predicted_iou = predicted_iou[m]\n",
        "        if (\n",
        "            curr_predicted_iou > max_predicted_iou\n",
        "            or selected_mask_using_predicted_iou is None\n",
        "        ):\n",
        "            max_predicted_iou = curr_predicted_iou\n",
        "            selected_mask_using_predicted_iou = all_masks[m]\n",
        "    return selected_mask_using_predicted_iou\n",
        "\n",
        "def run_ours_box(img_path, pts_sampled, model):\n",
        "    bbox = torch.reshape(torch.tensor(pts_sampled), [1, 1, 2, 2])\n",
        "    bbox_labels = torch.reshape(torch.tensor([2, 3]), [1, 1, 2])\n",
        "    image = cv2.imread(img_path)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    img_tensor = ToTensor()(image)\n",
        "\n",
        "    predicted_logits, predicted_iou = model(\n",
        "        img_tensor[None, ...].cuda(),\n",
        "        bbox.cuda(),\n",
        "        bbox_labels.cuda(),\n",
        "    )\n",
        "    predicted_logits = predicted_logits.cpu()\n",
        "    all_masks = torch.ge(torch.sigmoid(predicted_logits[0, 0, :, :, :]), 0.5).numpy()\n",
        "    predicted_iou = predicted_iou[0, 0, ...].cpu().detach().numpy()\n",
        "\n",
        "    max_predicted_iou = -1\n",
        "    selected_mask_using_predicted_iou = None\n",
        "    for m in range(all_masks.shape[0]):\n",
        "        curr_predicted_iou = predicted_iou[m]\n",
        "        if (\n",
        "            curr_predicted_iou > max_predicted_iou\n",
        "            or selected_mask_using_predicted_iou is None\n",
        "        ):\n",
        "            max_predicted_iou = curr_predicted_iou\n",
        "            selected_mask_using_predicted_iou = all_masks[m]\n",
        "    return selected_mask_using_predicted_iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-83WUeNPuJnT"
      },
      "source": [
        "#Visualization Related"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QKWt76-AG31h"
      },
      "outputs": [],
      "source": [
        "def show_mask(mask, ax, random_color=False):\n",
        "    if random_color:\n",
        "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
        "    else:\n",
        "        color = np.array([30/255, 144/255, 255/255, 0.8])\n",
        "    h, w = mask.shape[-2:]\n",
        "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
        "    ax.imshow(mask_image)\n",
        "\n",
        "def show_points(coords, labels, ax, marker_size=375):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='yellow', facecolor=(0,0,0,0), lw=5))\n",
        "\n",
        "def show_anns_ours(mask, ax):\n",
        "    ax.set_autoscale_on(False)\n",
        "    img = np.ones((mask[0].shape[0], mask[0].shape[1], 4))\n",
        "    img[:,:,3] = 0\n",
        "    for ann in mask:\n",
        "        m = ann\n",
        "        color_mask = np.concatenate([np.random.random(3), [0.5]])\n",
        "        img[m] = color_mask\n",
        "    ax.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHj10cGetlGN"
      },
      "source": [
        "#Load torchscript models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLPFPCuzB4lT"
      },
      "source": [
        "Download link for torchscript:\n",
        "\n",
        "EfficientSAM-s: https://www.dropbox.com/scl/fi/ziif8xudwbyyphb4tohza/efficientsam_s_gpu.jit?rlkey=8aflq9kf0bfujz5ex4lxuoq56&dl=0\n",
        "\n",
        "EfficientSAM-ti: https://www.dropbox.com/scl/fi/lup5s4gthmlv6qf3f5zz3/efficientsam_ti_gpu.jit?rlkey=pap1xktxw50qiaey17no16bqz&dl=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3HR4CCpYUpAI"
      },
      "outputs": [],
      "source": [
        "model = torch.jit.load('efficientsam_s_gpu.jit', map_location=\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b76-PTdKuidf"
      },
      "source": [
        "## Box segmentatoin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6ed2uLDCSDn"
      },
      "source": [
        "prepare your own image here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Xq7KloryJrhf"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/d2go/projects/sam/model/sam.py\", line 34, in forward\n      image_embeddings = image_embeddings0\n    else:\n      image_embeddings1 = (self).get_image_embeddings(batched_images, )\n                           ~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n      image_embeddings = image_embeddings1\n    H = self.H\n  File \"code/__torch__/d2go/projects/sam/model/sam.py\", line 127, in get_image_embeddings\n  def get_image_embeddings(self: __torch__.d2go.projects.sam.model.sam.Sam,\n    batched_images: Tensor) -> List[Tensor]:\n    batched_images1 = (self).preprocess(batched_images, )\n                       ~~~~~~~~~~~~~~~~ <--- HERE\n    image_encoder = self.image_encoder\n    _59 = (image_encoder).forward(batched_images1, )\n  File \"code/__torch__/d2go/projects/sam/model/sam.py\", line 241, in preprocess\n      x0 = x\n    pixel_mean = self.pixel_mean\n    _94 = torch.sub(x0, pixel_mean)\n          ~~~~~~~~~ <--- HERE\n    pixel_std = self.pixel_std\n    return torch.div(_94, pixel_std)\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/mnt/xarfuse/uid-462794/d7062d46-seed-7d5fdcd1-471c-4e32-b226-19878c868d15-ns-4026535078/d2go/projects/sam/model/sam.py\", line 646, in forward\n            )\n        else:\n            image_embeddings = self.get_image_embeddings(batched_images)\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    \n        return self.predict_masks(\n  File \"/mnt/xarfuse/uid-462794/d7062d46-seed-7d5fdcd1-471c-4e32-b226-19878c868d15-ns-4026535078/d2go/projects/sam/model/sam.py\", line 592, in get_image_embeddings\n          The last embedding corresponds to the final layer.\n        \"\"\"\n        batched_images = self.preprocess(batched_images)\n                         ~~~~~~~~~~~~~~~ <--- HERE\n        return self.image_encoder(batched_images)\n  File \"/mnt/xarfuse/uid-462794/d7062d46-seed-7d5fdcd1-471c-4e32-b226-19878c868d15-ns-4026535078/d2go/projects/sam/model/sam.py\", line 672, in preprocess\n                mode=\"bilinear\",\n            )\n        return (x - self.pixel_mean) / self.pixel_std\n                ~~~~~~~~~~~~~~~~~~~ <--- HERE\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-16-f7199bf695d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./figs/examples/demo_box.png'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmask_ours\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_ours_box\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_point\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-11-2572bb189cc5>\u001b[0m in \u001b[0;36mrun_ours_box\u001b[1;34m(img_path, pts_sampled, model)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mimg_tensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mbbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mbbox_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     )\n\u001b[0;32m     43\u001b[0m     \u001b[0mpredicted_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted_logits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32md:\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/d2go/projects/sam/model/sam.py\", line 34, in forward\n      image_embeddings = image_embeddings0\n    else:\n      image_embeddings1 = (self).get_image_embeddings(batched_images, )\n                           ~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n      image_embeddings = image_embeddings1\n    H = self.H\n  File \"code/__torch__/d2go/projects/sam/model/sam.py\", line 127, in get_image_embeddings\n  def get_image_embeddings(self: __torch__.d2go.projects.sam.model.sam.Sam,\n    batched_images: Tensor) -> List[Tensor]:\n    batched_images1 = (self).preprocess(batched_images, )\n                       ~~~~~~~~~~~~~~~~ <--- HERE\n    image_encoder = self.image_encoder\n    _59 = (image_encoder).forward(batched_images1, )\n  File \"code/__torch__/d2go/projects/sam/model/sam.py\", line 241, in preprocess\n      x0 = x\n    pixel_mean = self.pixel_mean\n    _94 = torch.sub(x0, pixel_mean)\n          ~~~~~~~~~ <--- HERE\n    pixel_std = self.pixel_std\n    return torch.div(_94, pixel_std)\n\nTraceback of TorchScript, original code (most recent call last):\n  File \"/mnt/xarfuse/uid-462794/d7062d46-seed-7d5fdcd1-471c-4e32-b226-19878c868d15-ns-4026535078/d2go/projects/sam/model/sam.py\", line 646, in forward\n            )\n        else:\n            image_embeddings = self.get_image_embeddings(batched_images)\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    \n        return self.predict_masks(\n  File \"/mnt/xarfuse/uid-462794/d7062d46-seed-7d5fdcd1-471c-4e32-b226-19878c868d15-ns-4026535078/d2go/projects/sam/model/sam.py\", line 592, in get_image_embeddings\n          The last embedding corresponds to the final layer.\n        \"\"\"\n        batched_images = self.preprocess(batched_images)\n                         ~~~~~~~~~~~~~~~ <--- HERE\n        return self.image_encoder(batched_images)\n  File \"/mnt/xarfuse/uid-462794/d7062d46-seed-7d5fdcd1-471c-4e32-b226-19878c868d15-ns-4026535078/d2go/projects/sam/model/sam.py\", line 672, in preprocess\n                mode=\"bilinear\",\n            )\n        return (x - self.pixel_mean) / self.pixel_std\n                ~~~~~~~~~~~~~~~~~~~ <--- HERE\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n"
          ]
        }
      ],
      "source": [
        "input_point = np.array([[100, 100], [300, 300]])\n",
        "input_label = np.array([1])\n",
        "image_path = './figs/examples/demo_box.png'\n",
        "\n",
        "mask_ours = run_ours_box(image_path, input_point, model)\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image)\n",
        "\n",
        "plt.figure(figsize=(20,20))\n",
        "plt.imshow(image)\n",
        "show_anns_ours(mask_ours, plt.gca())\n",
        "plt.title(f\"EfficientSAM\", fontsize=18)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IQBINppEQXW"
      },
      "source": [
        "## Point segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jpWTIwoNG452"
      },
      "outputs": [
        {
          "ename": "error",
          "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./figs/examples/demo_box.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      5\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(image_path)\n\u001b[1;32m----> 6\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(image, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB)\n\u001b[0;32m      8\u001b[0m mask \u001b[39m=\u001b[39m run_ours_point(image_path, input_point, model)\n\u001b[0;32m     10\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m,\u001b[39m10\u001b[39m))\n",
            "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
          ]
        }
      ],
      "source": [
        "input_point = np.array([[400, 400]])\n",
        "input_label = np.array([1])\n",
        "image_path = './figs/examples/demo_box.jpg'\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "mask = run_ours_point(image_path, input_point, model)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image)\n",
        "show_mask(mask, plt.gca())\n",
        "show_points(input_point, input_label, plt.gca())\n",
        "plt.title(f\"EfficientSAM\", fontsize=18)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
